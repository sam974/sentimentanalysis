{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "652003cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aperçu des 5 premières lignes du tableau de données :\n",
      "   sentiment          id                          date     query  \\\n",
      "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
      "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
      "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "\n",
      "              user                                               text  \n",
      "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
      "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
      "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
      "4           Karoli  @nationwideclass no, it's not behaving at all....  \n"
     ]
    }
   ],
   "source": [
    "# Import de la bibliothèque essentielle pour la manipulation de données\n",
    "import pandas as pd\n",
    "\n",
    "# On définit des noms de colonnes clairs\n",
    "column_names = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
    "\n",
    "# Chargement des données dans un \"DataFrame\"\n",
    "df = pd.read_csv(\"./input/training.1600000.processed.noemoticon.csv\", names=column_names, encoding=\"latin-1\")\n",
    "\n",
    "# On remplace la valeur 4 par 1 dans la colonne 'sentiment' pour la rendre binaire (0 ou 1)\n",
    "df['sentiment'] = df['sentiment'].replace(4, 1)\n",
    "\n",
    "# Affichage des 5 premières lignes pour un premier aperçu\n",
    "print(\"Aperçu des 5 premières lignes du tableau de données :\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mlflow-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow configuré pour l'expérience: 'Analyse de Sentiments - Twitter'\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.transformers\n",
    "\n",
    "# Configuration de l'URI de suivi pour MLflow\n",
    "# Les données seront sauvegardées dans le répertoire /mlflow à l'intérieur du conteneur Docker\n",
    "# qui est mappé sur /home/samuel/mlflow_data sur la machine hôte.\n",
    "# mlflow.set_tracking_uri(\"file:/home/samuel/mlflow\")\n",
    "mlflow.set_tracking_uri(\"file:/mlflow\")\n",
    "\n",
    "# Définition du nom de l'expérience\n",
    "experiment_name = \"Analyse de Sentiments - Twitter\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"MLflow configuré pour l'expérience: '{experiment_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3df60580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1600000.0</td>\n",
       "      <td>1.600000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.998818e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.935761e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.467810e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.956916e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2.002102e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.177059e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.329206e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment            id\n",
       "count  1600000.0  1.600000e+06\n",
       "mean         0.5  1.998818e+09\n",
       "std          0.5  1.935761e+08\n",
       "min          0.0  1.467810e+09\n",
       "25%          0.0  1.956916e+09\n",
       "50%          0.5  2.002102e+09\n",
       "75%          1.0  2.177059e+09\n",
       "max          1.0  2.329206e+09"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3241b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions du tableau : (1600000, 6)\n",
      "------------------------------\n",
      "Informations sur le DataFrame :\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count    Dtype \n",
      "---  ------     --------------    ----- \n",
      " 0   sentiment  1600000 non-null  int64 \n",
      " 1   id         1600000 non-null  int64 \n",
      " 2   date       1600000 non-null  object\n",
      " 3   query      1600000 non-null  object\n",
      " 4   user       1600000 non-null  object\n",
      " 5   text       1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n",
      "------------------------------\n",
      "Distribution des sentiments :\n",
      "sentiment\n",
      "0    800000\n",
      "1    800000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 1. Obtenir les dimensions du tableau (nombre de lignes, nombre de colonnes)\n",
    "print(f\"Dimensions du tableau : {df.shape}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 2. Obtenir un résumé des informations (types de données, valeurs non nulles)\n",
    "print(\"Informations sur le DataFrame :\")\n",
    "df.info()\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 3. Compter le nombre de tweets pour chaque sentiment\n",
    "# La colonne 'is_negative' contient notre label : 1 pour négatif, 0 pour non-négatif\n",
    "print(\"Distribution des sentiments :\")\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "281d2f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "Cleaned text:    httptwitpiccom2y1zl  awww thats a bummer  you shoulda got david carr of third day to do it d\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string # A pre-built list of all punctuation marks\n",
    "\n",
    "\n",
    "# This function removes the three main types of \"noise\"\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'https://\\S+|www\\.\\S+', '', text) # Remove URLs\n",
    "    text = re.sub(r'@\\w+', '', text)                # Remove mentions\n",
    "    text = re.sub(r'#', '', text)                   # Remove the '#' symbol\n",
    "    text = text.lower()                            # Convert to lowercase\n",
    "    # Remove all characters that are in the string.punctuation list\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text) \n",
    "    return text\n",
    "\n",
    "# We apply this function to every tweet in the 'text' column\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Let's check the result on one example\n",
    "print(\"Original text: \", df['text'].iloc[0])\n",
    "print(\"Cleaned text:  \", df['cleaned_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7951f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text:  httptwitpiccom2y1zl  awww thats a bummer  you shoulda got david carr of third day to do it d\n",
      "Final tokens: ['httptwitpiccom2y1zl', 'awww', 'thats', 'bummer', 'shoulda', 'got', 'david', 'carr', 'third', 'day']\n"
     ]
    }
   ],
   "source": [
    "# We'll use a popular library for this called NLTK\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# These lines download the necessary tools from NLTK (you only need to run them once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab') # Ajout suite à une LookupError\n",
    "\n",
    "# Load the list of English stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Apply tokenization and stop word removal\n",
    "df['tokens'] = df['cleaned_text'].apply(lambda text: [word for word in word_tokenize(text) if word not in stop_words])\n",
    "\n",
    "# Let's check our example one last time\n",
    "print(\"Cleaned text:\", df['cleaned_text'].iloc[0])\n",
    "print(\"Final tokens:\", df['tokens'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a6391e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of our feature matrix X: (1600000, 5000)\n",
      "Shape of our target vector y: (1600000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# The vectorizer needs sentences, so we join our tokens back into a single string\n",
    "df['final_text'] = df['tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "# We'll limit it to the 5,000 most important words to keep it efficient\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Create the numerical feature matrix X by applying the vectorizer\n",
    "X = vectorizer.fit_transform(df['final_text'])\n",
    "\n",
    "# Define our target variable y (the sentiment column)\n",
    "y = df['sentiment']\n",
    "\n",
    "# Check the final shapes\n",
    "print(\"Shape of our feature matrix X:\", X.shape)\n",
    "print(\"Shape of our target vector y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d18b388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (1280000, 5000)\n",
      "Testing data shape: (320000, 5000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split our data X and our labels y\n",
    "# test_size=0.2 means 20% of the data is reserved for testing\n",
    "# random_state=42 ensures we get the same split every time we run the code\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Let's print the shapes to see the result\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e4147e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage de l'expérimentation avec MLflow pour Logistic Regression...\n",
      "Training the model...\n",
      "Model training is complete!\n",
      "Making predictions on the test data...\n",
      "\n",
      "Overall Accuracy: 0.77\n",
      "\n",
      "Classification Report:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Non-négatif (0)       0.78      0.75      0.77    159494\n",
      "    Négatif (1)       0.76      0.80      0.78    160506\n",
      "\n",
      "       accuracy                           0.77    320000\n",
      "      macro avg       0.77      0.77      0.77    320000\n",
      "   weighted avg       0.77      0.77      0.77    320000\n",
      "\n",
      "\n",
      "Modèle et métriques enregistrés dans MLflow.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "with mlflow.start_run(run_name=\"Logistic Regression (TF-IDF)\") as run:\n",
    "    print(\"Démarrage de l'expérimentation avec MLflow pour Logistic Regression...\")\n",
    "    \n",
    "    # 1. Création et entraînement du modèle\n",
    "    model_lr = LogisticRegression(max_iter=1000)\n",
    "    print(\"Training the model...\")\n",
    "    model_lr.fit(X_train, y_train)\n",
    "    print(\"Model training is complete!\")\n",
    "\n",
    "    # 2. Prédictions et évaluation\n",
    "    print(\"Making predictions on the test data...\")\n",
    "    y_pred = model_lr.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=['Non-négatif (0)', 'Négatif (1)'], output_dict=True)\n",
    "    \n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.2f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Non-négatif (0)', 'Négatif (1)']))\n",
    "\n",
    "    # 3. Suivi avec MLflow\n",
    "    mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"vectorizer\", \"TfidfVectorizer\")\n",
    "    mlflow.log_param(\"tfidf_max_features\", 5000)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    # Nettoyage des noms de métriques pour MLflow\n",
    "    # Les noms ne doivent pas contenir de caractères spéciaux comme '()' ou des accents\n",
    "    cleaned_report = {}\n",
    "    for key, value in report.items():\n",
    "        if isinstance(value, dict):\n",
    "            cleaned_key = key.replace(' (0)', '_0').replace(' (1)', '_1').replace('-', '_')\n",
    "            cleaned_report[cleaned_key] = value\n",
    "        else:\n",
    "            cleaned_report[key] = value\n",
    "    mlflow.log_metrics({f\"precision_{k}\": v['precision'] for k, v in cleaned_report.items() if isinstance(v, dict)})\n",
    "    mlflow.log_metrics({f\"recall_{k}\": v['recall'] for k, v in cleaned_report.items() if isinstance(v, dict)})\n",
    "    mlflow.log_metrics({f\"f1_score_{k}\": v['f1-score'] for k, v in cleaned_report.items() if isinstance(v, dict)})\n",
    "    # Sauvegarde du rapport de classification\n",
    "    mlflow.log_text(classification_report(y_test, y_pred, target_names=['Non-négatif (0)', 'Négatif (1)']), \"classification_report.txt\")\n",
    "    \n",
    "    # Sauvegarde du modèle avec une signature et un exemple d'entrée\n",
    "    # Cela aide MLflow à comprendre le format des données attendues et produites\n",
    "    from mlflow.models.signature import infer_signature\n",
    "    signature = infer_signature(X_train, y_pred)\n",
    "    input_example = X_train[:5] # Un petit échantillon des données d'entrée\n",
    "    mlflow.sklearn.log_model(model_lr, name=\"logistic_regression_model\", signature=signature, input_example=input_example)\n",
    "    \n",
    "    print(\"\\nModèle et métriques enregistrés dans MLflow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a217cef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-21 12:23:26.798198: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761049406.971835   26104 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761049407.036225   26104 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1761049407.430917   26104 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761049407.430989   26104 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761049407.430999   26104 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1761049407.431007   26104 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-10-21 12:23:27.481018: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistilBERT model and its tokenizer are loaded and ready!\n"
     ]
    }
   ],
   "source": [
    "# We'll use the same powerful libraries as before\n",
    "import os\n",
    "\n",
    "# Disable the progress bar from huggingface_hub to avoid LookupError in some notebook environments\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# The correct, English-based model name\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# 1. Load the Tokenizer for DistilBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 2. Load the DistilBERT model for sequence classification (with 2 labels)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "print(\"DistilBERT model and its tokenizer are loaded and ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cf7b732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:\n",
      " i love the new flight attendant service\n",
      "\n",
      "Token IDs (input_ids):\n",
      " tensor([[  101,  1045,  2293,  1996,  2047,  3462, 16742,  2326,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0]])\n",
      "\n",
      "Attention Mask:\n",
      " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "# A sample tweet from our dataset\n",
    "sample_text = \"i love the new flight attendant service\"\n",
    "\n",
    "# Use the tokenizer to process the text\n",
    "# padding='max_length' and truncation=True ensure all sentences are the same size\n",
    "encoding = tokenizer(sample_text, \n",
    "                     padding='max_length', \n",
    "                     truncation=True, \n",
    "                     max_length=64, # A common length for tweets\n",
    "                     return_tensors='pt') # Return as PyTorch tensors\n",
    "\n",
    "print(\"Original Sentence:\\n\", sample_text)\n",
    "print(\"\\nToken IDs (input_ids):\\n\", encoding['input_ids'])\n",
    "print(\"\\nAttention Mask:\\n\", encoding['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20182ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Let's create a smaller, manageable sample of our data for this demo\n",
    "sample_df = df.sample(n=16000, random_state=42)\n",
    "\n",
    "# Create an instance of our custom Dataset\n",
    "MAX_LEN = 64\n",
    "sentiment_dataset = SentimentDataset(\n",
    "    texts=sample_df.text.to_numpy(),\n",
    "    labels=sample_df.sentiment.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49946c16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in the training loader: 800\n",
      "Number of batches in the testing loader: 200\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "\n",
    "# 1. Split the main dataset\n",
    "train_size = int(0.8 * len(sentiment_dataset))\n",
    "test_size = len(sentiment_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(sentiment_dataset, [train_size, test_size])\n",
    "\n",
    "# 2. Create the DataLoaders for each set\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "  train_dataset,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=True\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "  test_dataset,\n",
    "  batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in the training loader: {len(train_data_loader)}\")\n",
    "print(f\"Number of batches in the testing loader: {len(test_data_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82e595f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277f902689a14e4d93b24554b33d8dd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): DistilBertSdpaAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm # For a nice progress bar\n",
    "\n",
    "# 1. SETUP\n",
    "# Move model to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Initialize the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5) # lr is the learning rate\n",
    "\n",
    "# 2. TRAINING LOOP\n",
    "model.train() # Set the model to training mode\n",
    "for batch in tqdm(train_data_loader):\n",
    "    # Move batch data to the correct device\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    labels = batch['labels'].to(device)\n",
    "\n",
    "    # Forward pass: get model outputs\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "    \n",
    "    # Get the loss (the measure of error)\n",
    "    loss = outputs.loss\n",
    "    \n",
    "    # Backward pass: calculate gradients\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "# 3. EVALUATION LOOP\n",
    "model.eval() # Set the model to evaluation mode\n",
    "# ... similar loop over the test_data_loader, but without the optimizer steps ...\n",
    "# We would collect all predictions and then calculate accuracy, precision, and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "63b9497e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 1 / 2 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e704b2c8ee2e47209b45115cb6a57929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "--- Epoch 2 / 2 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe65bb1d36e64ad4bba142e9d1946f97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8272c16d62840ad9b51d614426e1dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Résultats pour l'Epoch 2:\n",
      "{'Non-négatif (0)': {'precision': 0.8144853875476493, 'recall': 0.7972636815920398, 'f1-score': 0.8057825267127593, 'support': 1608.0}, 'Négatif (1)': {'precision': 0.7995079950799509, 'recall': 0.8165829145728644, 'f1-score': 0.8079552517091361, 'support': 1592.0}, 'accuracy': 0.806875, 'macro avg': {'precision': 0.8069966913138, 'recall': 0.8069232980824521, 'f1-score': 0.8068688892109477, 'support': 3200.0}, 'weighted avg': {'precision': 0.8070341347949693, 'recall': 0.806875, 'f1-score': 0.8068634573984568, 'support': 3200.0}}\n",
      "Logged DistilBERT metrics to MLflow (run_id=d18ed8ea384f4c7f9578af0bf9b8db81) - accuracy=0.7736\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "write() argument must be str, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 120\u001b[39m\n\u001b[32m    116\u001b[39m mlflow.log_text(classification_report(true_labels, predictions, target_names=[\u001b[33m'\u001b[39m\u001b[33mNon-négatif (0)\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNégatif (1)\u001b[39m\u001b[33m'\u001b[39m]), \u001b[33m\"\u001b[39m\u001b[33mclassification_report_distilbert.txt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    118\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLogged DistilBERT metrics to MLflow (run_id=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun.info.run_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m) - accuracy=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreport\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mclassification_report_epoch_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch\u001b[49m\u001b[43m+\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;66;03m# Sauvegarde du modèle à la fin de chaque époque (ou seulement le meilleur)\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Pour cet exemple, nous sauvegardons le modèle de la dernière époque.\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m epoch == EPOCHS - \u001b[32m1\u001b[39m:\n\u001b[32m    125\u001b[39m     \u001b[38;5;66;03m# Création d'un dictionnaire pour le modèle et le tokenizer\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/mlflow/tracking/fluent.py:1517\u001b[39m, in \u001b[36mlog_text\u001b[39m\u001b[34m(text, artifact_file, run_id)\u001b[39m\n\u001b[32m   1489\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1490\u001b[39m \u001b[33;03mLog text as an artifact.\u001b[39;00m\n\u001b[32m   1491\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1514\u001b[39m \n\u001b[32m   1515\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1516\u001b[39m run_id = run_id \u001b[38;5;129;01mor\u001b[39;00m _get_or_start_run().info.run_id\n\u001b[32m-> \u001b[39m\u001b[32m1517\u001b[39m \u001b[43mMlflowClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlog_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/site-packages/mlflow/tracking/client.py:2649\u001b[39m, in \u001b[36mMlflowClient.log_text\u001b[39m\u001b[34m(self, run_id, text, artifact_file)\u001b[39m\n\u001b[32m   2647\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._log_artifact_helper(run_id, artifact_file) \u001b[38;5;28;01mas\u001b[39;00m tmp_path:\n\u001b[32m   2648\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(tmp_path, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m-> \u001b[39m\u001b[32m2649\u001b[39m         \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: write() argument must be str, not dict"
     ]
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm # Pour une belle barre de progression\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Listes pour stocker les métriques\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'accuracy': []\n",
    "}\n",
    "\n",
    "# 1. PRÉPARATION\n",
    "# Déplace le modèle sur le GPU si disponible, sinon le CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Initialise l'optimiseur, qui met à jour les poids du modèle\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Nombre d'époques (cycles d'entraînement complets)\n",
    "EPOCHS = 2\n",
    "\n",
    "# 2. BOUCLE D'ENTRAÎNEMENT PRINCIPALE\n",
    "with mlflow.start_run(run_name=\"DistilBERT\") as run:\n",
    "    # Log des hyperparamètres\n",
    "    mlflow.log_params({\"model_name\": model_name, \"epochs\": EPOCHS, \"learning_rate\": 2e-5, \"batch_size\": BATCH_SIZE, \"max_len\": MAX_LEN})\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'--- Epoch {epoch + 1} / {EPOCHS} ---')\n",
    "        \n",
    "        # -- Phase d'entraînement --\n",
    "        model.train() # Passe le modèle en mode \"entraînement\"\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_data_loader, desc=\"Training\"):\n",
    "            # Déplace les données du batch sur le bon appareil (CPU ou GPU)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Réinitialise les gradients de l'itération précédente\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Le modèle traite les données et calcule la perte (l'erreur)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Rétropropagation : calcule comment ajuster les poids pour réduire l'erreur\n",
    "            loss.backward()\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            # L'optimiseur applique les ajustements aux poids du modèle\n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_data_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "        # Log de la perte d'entraînement pour l'époque\n",
    "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
    "\n",
    "        # -- Phase d'évaluation --\n",
    "        print(\"Evaluating...\")\n",
    "        model.eval() # Passe le modèle en mode \"évaluation\"\n",
    "    \n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    with torch.no_grad(): # Pas besoin de calculer les gradients pendant l'évaluation\n",
    "        for batch in tqdm(test_data_loader, desc=\"Evaluating\"):\n",
    "            # Déplace les données du batch sur le bon appareil\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # Le modèle fait ses prédictions\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            # On récupère la prédiction la plus probable (0 ou 1)\n",
    "            logits = outputs.logits\n",
    "            batch_predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            predictions.extend(batch_predictions.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_accuracy = accuracy_score(true_labels, predictions)\n",
    "    history['accuracy'].append(val_accuracy)\n",
    "    \n",
    "    # Log de la précision de validation pour l'époque\n",
    "    mlflow.log_metric(\"accuracy\", val_accuracy, step=epoch)\n",
    "\n",
    "    # Affiche le rapport de classification pour l'époch en cours\n",
    "    # Sauvegarde du rapport de l'époque\n",
    "    report = classification_report(true_labels, predictions, target_names=['Non-négatif (0)', 'Négatif (1)'], output_dict=True)\n",
    "    print(f\"\\nRésultats pour l'Epoch {epoch + 1}:\")\n",
    "    print(report)\n",
    "\n",
    "    # Paramètres basiques\n",
    "    mlflow.log_param(\"model_type\", \"DistilBERT\")\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    # Nettoyage des clés pour MLflow (pas de caractères spéciaux)\n",
    "    cleaned_report = {}\n",
    "    for key, value in report.items():\n",
    "        if isinstance(value, dict):\n",
    "            cleaned_key = key.replace(' (0)', '_0').replace(' (1)', '_1').replace('-', '_')\n",
    "            cleaned_report[cleaned_key] = value\n",
    "        else:\n",
    "            cleaned_report[key] = value\n",
    "\n",
    "    # Log des métriques par classe (precision, recall, f1)\n",
    "    mlflow.log_metrics({f\"precision_{k}\": v['precision'] for k, v in cleaned_report.items() if isinstance(v, dict)})\n",
    "    mlflow.log_metrics({f\"recall_{k}\": v['recall'] for k, v in cleaned_report.items() if isinstance(v, dict)})\n",
    "    mlflow.log_metrics({f\"f1_score_{k}\": v['f1-score'] for k, v in cleaned_report.items() if isinstance(v, dict)})\n",
    "\n",
    "    # Sauvegarde du rapport complet en texte\n",
    "    mlflow.log_text(classification_report(true_labels, predictions, target_names=['Non-négatif (0)', 'Négatif (1)']), \"classification_report_distilbert.txt\")\n",
    "\n",
    "    print(f\"Logged DistilBERT metrics to MLflow (run_id={run.info.run_id}) - accuracy={accuracy:.4f}\")\n",
    "\n",
    "    # mlflow.log_text(report, f\"classification_report_epoch_{epoch+1}.txt\")\n",
    "\n",
    "    # Sauvegarde du modèle à la fin de chaque époque (ou seulement le meilleur)\n",
    "    # Pour cet exemple, nous sauvegardons le modèle de la dernière époque.\n",
    "    if epoch == EPOCHS - 1:\n",
    "        # Création d'un dictionnaire pour le modèle et le tokenizer\n",
    "        model_artifact = {\"model\": model, \"tokenizer\": tokenizer}\n",
    "        # Log du modèle avec le flavor 'transformers'\n",
    "        mlflow.transformers.log_model(\n",
    "            transformers_model=model_artifact,\n",
    "            name=\"distilbert_sentiment_model\"\n",
    "        )\n",
    "\n",
    "print(\"\\nEntraînement terminé et modèle sauvegardé avec MLflow !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb0029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Créer une figure avec deux graphiques\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Graphique de la perte d'entraînement\n",
    "ax1.plot(history['train_loss'], label='Perte d\\'entraînement')\n",
    "ax1.set_title('Évolution de la Perte')\n",
    "ax1.set_xlabel('Épochs')\n",
    "ax1.set_ylabel('Perte')\n",
    "ax1.legend()\n",
    "\n",
    "# Graphique de la précision de validation\n",
    "ax2.plot(history['accuracy'], label='Précision de validation', color='orange')\n",
    "ax2.set_title('Évolution de la Précision')\n",
    "ax2.set_xlabel('Épochs')\n",
    "ax2.set_ylabel('Précision')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4297669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1. Définir le chemin du répertoire où tout sera sauvegardé\n",
    "output_dir = \"./sentiment_distilbert_model/\"\n",
    "\n",
    "# Crée le répertoire s'il n'existe pas\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Sauvegarde du modèle dans le répertoire {output_dir}\")\n",
    "\n",
    "# 2. Sauvegarder le modèle fine-tuné\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "# 3. Sauvegarder le tokenizer associé\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"\\nModèle et tokenizer sauvegardés avec succès !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ef5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# 1. Définir le chemin où se trouve le modèle sauvegardé\n",
    "output_dir = \"./sentiment_distilbert_model/\"\n",
    "\n",
    "# 2. Charger le tokenizer depuis ce répertoire\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# 3. Charger le modèle depuis ce répertoire\n",
    "model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
    "\n",
    "# Mettons le modèle en mode évaluation (important !)\n",
    "model.eval()\n",
    "\n",
    "# 4. Utiliser le modèle pour une nouvelle prédiction\n",
    "new_tweet = \"The flight was surprisingly on time and the crew was wonderful.\"\n",
    "\n",
    "# Préparer le tweet avec le tokenizer (comme pendant l'entraînement)\n",
    "inputs = tokenizer(new_tweet, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "# Faire la prédiction\n",
    "with torch.no_grad(): # Pas besoin de calculer les gradients pour une simple prédiction\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Obtenir le résultat\n",
    "logits = outputs.logits\n",
    "prediction = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "# Interpréter le résultat\n",
    "sentiments = ['Négatif (0)', 'Positif (1)']\n",
    "print(f\"Le tweet : '{new_tweet}'\")\n",
    "print(f\"Result: {prediction}\")\n",
    "print(f\"Prédiction : {sentiments[prediction]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mlflow-serving-title",
   "metadata": {},
   "source": [
    "## 5. Chargement et Service du Modèle avec MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mlflow-serving-intro",
   "metadata": {},
   "source": [
    "Maintenant que nous avons entraîné et sauvegardé nos modèles avec MLflow, voyons comment les réutiliser. Nous allons charger le modèle DistilBERT (qui est le plus performant) depuis le tracking server de MLflow et l'utiliser pour faire une prédiction. Ensuite, nous verrons comment le déployer en tant que service local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlflow-load-predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "# Récupérer la dernière exécution (run) de l'expérience\n",
    "runs = mlflow.search_runs(experiment_names=[experiment_name], order_by=[\"start_time DESC\"], max_results=1)\n",
    "last_run_id = runs.iloc[0]['run_id']\n",
    "\n",
    "# Construire l'URI du modèle\n",
    "model_uri = f\"runs:/{last_run_id}/distilbert_sentiment_model\"\n",
    "\n",
    "# Charger le modèle en tant que fonction PyFunc\n",
    "# PyFunc est un format générique qui permet à MLflow de servir n'importe quel modèle Python\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "# Préparer une nouvelle donnée pour la prédiction\n",
    "new_tweet = \"This is the worst flight experience I have ever had.\"\n",
    "prediction_data = pd.DataFrame([new_tweet], columns=[\"text\"])\n",
    "\n",
    "# Faire une prédiction\n",
    "prediction = loaded_model.predict(prediction_data)\n",
    "\n",
    "sentiments = ['Négatif (0)', 'Positif (1)']\n",
    "print(f\"Tweet: '{new_tweet}'\")\n",
    "print(f\"Prédiction : {sentiments[prediction[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mlflow-serving-cli",
   "metadata": {},
   "source": [
    "### Démarrer un serveur de prédiction local\n",
    "\n",
    "MLflow facilite également le déploiement d'un modèle en tant que service REST API. Pour démarrer un serveur local qui sert notre modèle DistilBERT, vous pouvez exécuter la commande suivante dans votre terminal (en vous assurant que l'environnement où MLflow est installé est activé) :\n",
    "\n",
    "```bash\n",
    "# La variable MLFLOW_TRACKING_URI doit pointer vers votre serveur de suivi\n",
    "export MLFLOW_TRACKING_URI=file:///home/samuel/mlflow_data\n",
    "\n",
    "# Servez le modèle à partir de son URI\n",
    "mlflow models serve -m \"runs:/<RUN_ID>/distilbert_sentiment_model\" -p 1234\n",
    "```\n",
    "\n",
    "Remplacez `<RUN_ID>` par l'ID de l'exécution que vous souhaitez servir (par exemple, celui que nous avons récupéré dans la cellule précédente). Une fois le serveur démarré, vous pouvez lui envoyer des requêtes POST pour obtenir des prédictions, par exemple avec `curl` :\n",
    "\n",
    "```bash\n",
    "curl -X POST -H \"Content-Type:application/json\" --data '{\"dataframe_split\": {\"columns\":[\"text\"], \"data\":[[\"I love MLflow!\"]]}}' http://127.0.0.1:1234/invocations\n",
    "```\n",
    "\n",
    "Cela conclut notre tour d'horizon de l'intégration de MLflow pour le suivi, la gestion et le déploiement de modèles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
