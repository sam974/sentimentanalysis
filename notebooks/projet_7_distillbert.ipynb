{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "652003cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aperçu des 5 premières lignes du tableau de données :\n",
      "   sentiment          id                          date     query  \\\n",
      "0          0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
      "1          0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
      "2          0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
      "3          0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "4          0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
      "\n",
      "              user                                               text  \n",
      "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
      "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
      "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
      "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
      "4           Karoli  @nationwideclass no, it's not behaving at all....  \n"
     ]
    }
   ],
   "source": [
    "# Import de la bibliothèque essentielle pour la manipulation de données\n",
    "import pandas as pd\n",
    "\n",
    "# On définit des noms de colonnes clairs\n",
    "column_names = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
    "\n",
    "# Chargement des données dans un \"DataFrame\"\n",
    "df = pd.read_csv(\"./input/training.1600000.processed.noemoticon.csv\", names=column_names, encoding=\"latin-1\")\n",
    "\n",
    "# On remplace la valeur 4 par 1 dans la colonne 'sentiment' pour la rendre binaire (0 ou 1)\n",
    "df['sentiment'] = df['sentiment'].replace(4, 1)\n",
    "\n",
    "# Affichage des 5 premières lignes pour un premier aperçu\n",
    "print(\"Aperçu des 5 premières lignes du tableau de données :\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mlflow-setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow configuré pour l'expérience: 'Analyse de Sentiments - Twitter'\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.transformers\n",
    "\n",
    "# Configuration de l'URI de suivi pour MLflow\n",
    "mlflow.set_tracking_uri(\"file:/mlflow\")\n",
    "\n",
    "# Définition du nom de l'expérience\n",
    "experiment_name = \"Analyse de Sentiments - Twitter\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"MLflow configuré pour l'expérience: '{experiment_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3df60580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1600000.0</td>\n",
       "      <td>1.600000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.998818e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.5</td>\n",
       "      <td>1.935761e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.467810e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.956916e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.5</td>\n",
       "      <td>2.002102e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.177059e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2.329206e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentiment            id\n",
       "count  1600000.0  1.600000e+06\n",
       "mean         0.5  1.998818e+09\n",
       "std          0.5  1.935761e+08\n",
       "min          0.0  1.467810e+09\n",
       "25%          0.0  1.956916e+09\n",
       "50%          0.5  2.002102e+09\n",
       "75%          1.0  2.177059e+09\n",
       "max          1.0  2.329206e+09"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7568ba7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of truncated and stratified DataFrame:\n",
      "(16000, 2)\n",
      "\n",
      "Aperçu du DataFrame tronqué et stratifié:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@pbadstibner I have good balance..used to do m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>@gtissa Still having issue and it's GDI!!! The...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>@Chrismorris528 Sigh. In 3 hours. It sucks to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>@HelloEli exacly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>In fairness. He smells good.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentiment                                               text\n",
       "0          1  @pbadstibner I have good balance..used to do m...\n",
       "1          0  @gtissa Still having issue and it's GDI!!! The...\n",
       "2          0  @Chrismorris528 Sigh. In 3 hours. It sucks to ...\n",
       "3          0                                  @HelloEli exacly \n",
       "4          1                      In fairness. He smells good. "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Value counts of sentiment in truncated and stratified DataFrame:\n",
      "sentiment\n",
      "1    8000\n",
      "0    8000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Reduce the DataFrame to 16000 elements, stratified by column 0\n",
    "n_samples = 16000\n",
    "\n",
    "# Sample 8000 rows where sentiment is 0 (negative sentiment)\n",
    "df_neg = df[df['sentiment'] == 0].sample(n=n_samples // 2, random_state=42)\n",
    "\n",
    "# Sample 8000 rows where sentiment is 1 (positive sentiment)\n",
    "df_pos = df[df['sentiment'] == 1].sample(n=n_samples // 2, random_state=42)\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "df_truncated_stratified = pd.concat([df_neg, df_pos])\n",
    "\n",
    "# Shuffle the truncated DataFrame\n",
    "df_truncated_stratified = df_truncated_stratified.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Select only the columns we need (sentiment and text)\n",
    "df_prepared = df_truncated_stratified[['sentiment', 'text']]\n",
    "\n",
    "\n",
    "# Display the shape of the truncated and stratified DataFrame\n",
    "print(\"Shape of truncated and stratified DataFrame:\")\n",
    "print(df_prepared.shape)\n",
    "\n",
    "# Display the first few rows of the truncated and stratified DataFrame\n",
    "print(\"\\nAperçu du DataFrame tronqué et stratifié:\")\n",
    "display(df_prepared.head())\n",
    "\n",
    "# Check the value counts of the sentiment column to confirm stratification\n",
    "print(\"\\nValue counts of sentiment in truncated and stratified DataFrame:\")\n",
    "print(df_prepared['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3241b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions du tableau : (1600000, 6)\n",
      "------------------------------\n",
      "Informations sur le DataFrame :\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count    Dtype \n",
      "---  ------     --------------    ----- \n",
      " 0   sentiment  1600000 non-null  int64 \n",
      " 1   id         1600000 non-null  int64 \n",
      " 2   date       1600000 non-null  object\n",
      " 3   query      1600000 non-null  object\n",
      " 4   user       1600000 non-null  object\n",
      " 5   text       1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n",
      "------------------------------\n",
      "Distribution des sentiments :\n",
      "sentiment\n",
      "0    800000\n",
      "1    800000\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df_prepared\n",
    "# 1. Obtenir les dimensions du tableau (nombre de lignes, nombre de colonnes)\n",
    "print(f\"Dimensions du tableau : {df.shape}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 2. Obtenir un résumé des informations (types de données, valeurs non nulles)\n",
    "print(\"Informations sur le DataFrame :\")\n",
    "df.info()\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 3. Compter le nombre de tweets pour chaque sentiment\n",
    "print(\"Distribution des sentiments :\")\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "281d2f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:   @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D\n",
      "Cleaned text:     httptwitpiccom2y1zl  awww thats a bummer  you shoulda got david carr of third day to do it d\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'https://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(f'[{re.escape(string.punctuation)}]', '', text) \n",
    "    return text\n",
    "\n",
    "df['cleaned_text'] = df['text'].apply(clean_text)\n",
    "\n",
    "print(\"Original text:  \", df['text'].iloc[0])\n",
    "print(\"Cleaned text:   \", df['cleaned_text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7951f3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Moi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Moi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "df['tokens'] = df['cleaned_text'].apply(lambda text: [word for word in word_tokenize(text) if word not in stop_words])\n",
    "\n",
    "print(\"Cleaned text:\", df['cleaned_text'].iloc[0])\n",
    "print(\"Final tokens:\", df['tokens'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6391e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "df['final_text'] = df['tokens'].apply(lambda tokens: ' '.join(tokens))\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "X = vectorizer.fit_transform(df['final_text'])\n",
    "\n",
    "y = df['sentiment']\n",
    "\n",
    "print(\"Shape of our feature matrix X:\", X.shape)\n",
    "print(\"Shape of our target vector y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d18b388",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Testing data shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4147e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "with mlflow.start_run(run_name=\"Logistic Regression (TF-IDF)\") as run:\n",
    "    print(\"Démarrage de l'expérimentation avec MLflow pour Logistic Regression...\")\n",
    "    \n",
    "    model_lr = LogisticRegression(max_iter=1000)\n",
    "    print(\"Training the model...\")\n",
    "    model_lr.fit(X_train, y_train)\n",
    "    print(\"Model training is complete!\")\n",
    "\n",
    "    print(\"Making predictions on the test data...\")\n",
    "    y_pred = model_lr.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, target_names=['Non-négatif (0)', 'Négatif (1)'], output_dict=True)\n",
    "    \n",
    "    print(f\"\\nOverall Accuracy: {accuracy:.2f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Non-négatif (0)', 'Négatif (1)']))\n",
    "\n",
    "    mlflow.log_param(\"model_type\", \"LogisticRegression\")\n",
    "    mlflow.log_param(\"vectorizer\", \"TfidfVectorizer\")\n",
    "    mlflow.log_param(\"tfidf_max_features\", 5000)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    cleaned_report = {}\n",
    "    for key, value in report.items():\n",
    "        if isinstance(value, dict):\n",
    "            cleaned_key = key.replace(' (0)', '_0').replace(' (1)', '_1').replace('-', '_')\n",
    "            cleaned_report[cleaned_key] = value\n",
    "        else:\n",
    "            cleaned_report[key] = value\n",
    "    mlflow.log_metrics({f\"precision_{k}\": v['precision'] for k, v in cleaned_report.items() if isinstance(v, dict)})\n",
    "    mlflow.log_metrics({f\"recall_{k}\": v['recall'] for k, v in cleaned_report.items() if isinstance(v, dict)})\n",
    "    mlflow.log_metrics({f\"f1_score_{k}\": v['f1-score'] for k, v in cleaned_report.items() if isinstance(v, dict)})\n",
    "    mlflow.log_text(classification_report(y_test, y_pred, target_names=['Non-négatif (0)', 'Négatif (1)']), \"classification_report.txt\")\n",
    "    \n",
    "    from mlflow.models.signature import infer_signature\n",
    "    signature = infer_signature(X_train, y_pred)\n",
    "    input_example = X_train[:5]\n",
    "    mlflow.sklearn.log_model(model_lr, \"logistic_regression_model\", signature=signature, input_example=input_example)\n",
    "    \n",
    "    print(\"\\nModèle et métriques enregistrés dans MLflow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "glove-lstm-cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for the GloVe model\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "import mlflow\n",
    "import mlflow.keras\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import os\n",
    "\n",
    "with mlflow.start_run(run_name=\"GloVe_LSTM\") as run:\n",
    "    print(\"Starting GloVe LSTM experiment with MLflow...\")\n",
    "\n",
    "    # --- 1. Data Preparation for GloVe ---\n",
    "    glove_file_path = r'C:\\formations\\ingénieur ia\\projet 7\\dev_bis\\sentimentanalysis\\input\\glove.6B\\glove.6B.100d.txt'\n",
    "    embedding_dim = 100\n",
    "    max_features = 10000\n",
    "    max_len = 100\n",
    "\n",
    "    tokenizer_glove = Tokenizer(num_words=max_features, split=' ')\n",
    "    tokenizer_glove.fit_on_texts(df['tokens'].values)\n",
    "    X_glove = tokenizer_glove.texts_to_sequences(df['tokens'].values)\n",
    "    X_glove = pad_sequences(X_glove, maxlen=max_len)\n",
    "    y_glove = df['sentiment'].values\n",
    "\n",
    "    X_train_glove, X_test_glove, y_train_glove, y_test_glove = train_test_split(X_glove, y_glove, test_size=0.2, random_state=42)\n",
    "\n",
    "    print(f\"Training data shape: {X_train_glove.shape}\")\n",
    "    print(f\"Testing data shape: {X_test_glove.shape}\")\n",
    "\n",
    "    # --- 2. Load GloVe Embeddings ---\n",
    "    embeddings_index = {}\n",
    "    try:\n",
    "        with open(glove_file_path, encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                values = line.split()\n",
    "                word = values[0]\n",
    "                coefs = np.asarray(values[1:], dtype='float32')\n",
    "                embeddings_index[word] = coefs\n",
    "    except FileNotFoundError:\n",
    "        print(f\"GloVe file not found at {glove_file_path}. Please check the path.\")\n",
    "        mlflow.end_run()\n",
    "\n",
    "    # --- 3. Create Embedding Matrix ---\n",
    "    word_index = tokenizer_glove.word_index\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    # --- 4. Build the LSTM Model ---\n",
    "    model_glove = Sequential()\n",
    "    model_glove.add(Embedding(len(word_index) + 1,\n",
    "                              embedding_dim,\n",
    "                              weights=[embedding_matrix],\n",
    "                              input_length=max_len,\n",
    "                              trainable=False))\n",
    "    model_glove.add(SpatialDropout1D(0.2))\n",
    "    model_glove.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model_glove.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model_glove.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model_glove.summary())\n",
    "\n",
    "    # --- 5. Train the Model ---\n",
    "    epochs = 3\n",
    "    batch_size = 64\n",
    "\n",
    "    history = model_glove.fit(X_train_glove, y_train_glove,\n",
    "                              epochs=epochs,\n",
    "                              batch_size=batch_size,\n",
    "                              validation_data=(X_test_glove, y_test_glove),\n",
    "                              verbose=1)\n",
    "\n",
    "    # --- 6. Evaluate and Log with MLflow ---\n",
    "    y_pred_glove_proba = model_glove.predict(X_test_glove)\n",
    "    y_pred_glove = (y_pred_glove_proba > 0.5).astype(\"int32\")\n",
    "\n",
    "    accuracy_glove = accuracy_score(y_test_glove, y_pred_glove)\n",
    "    report_glove = classification_report(y_test_glove, y_pred_glove, target_names=['Non-négatif (0)', 'Négatif (1)'], output_dict=True)\n",
    "\n",
    "    print(f\"\\nOverall Accuracy (GloVe): {accuracy_glove:.2f}\")\n",
    "    print(\"\\nClassification Report (GloVe):\")\n",
    "    print(classification_report(y_test_glove, y_pred_glove, target_names=['Non-négatif (0)', 'Négatif (1)']))\n",
    "\n",
    "    mlflow.log_param(\"model_type\", \"GloVe_LSTM\")\n",
    "    mlflow.log_param(\"embedding_dim\", embedding_dim)\n",
    "    mlflow.log_param(\"max_features\", max_features)\n",
    "    mlflow.log_param(\"max_len\", max_len)\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_glove)\n",
    "    \n",
    "    cleaned_report_glove = {}\n",
    "    for key, value in report_glove.items():\n",
    "        if isinstance(value, dict):\n",
    "            cleaned_key = key.replace(' (0)', '_0').replace(' (1)', '_1').replace('-', '_')\n",
    "            cleaned_report_glove[cleaned_key] = value\n",
    "        else:\n",
    "            cleaned_report_glove[key] = value\n",
    "\n",
    "    mlflow.log_metrics({f\"precision_{k}\": v['precision'] for k, v in cleaned_report_glove.items() if isinstance(v, dict)})\n",
    "    mlflow.log_metrics({f\"recall_{k}\": v['recall'] for k, v in cleaned_report_glove.items() if isinstance(v, dict)})\n",
    "    mlflow.log_metrics({f\"f1_score_{k}\": v['f1-score'] for k, v in cleaned_report_glove.items() if isinstance(v, dict)})\n",
    "    \n",
    "    mlflow.log_text(classification_report(y_test_glove, y_pred_glove, target_names=['Non-négatif (0)', 'Négatif (1)']), \"classification_report.txt\")\n",
    "\n",
    "    mlflow.keras.log_model(model_glove, \"glove_lstm_model\")\n",
    "\n",
    "    print(\"\\nGloVe LSTM model and metrics logged to MLflow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the same powerful libraries as before\n",
    "import os\n",
    "\n",
    "# Disable the progress bar from huggingface_hub to avoid LookupError in some notebook environments\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# The correct, English-based model name\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# 1. Load the Tokenizer for DistilBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 2. Load the DistilBERT model for sequence classification (with 2 labels)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "print(\"DistilBERT model and its tokenizer are loaded and ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf7b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample tweet from our dataset\n",
    "sample_text = \"i love the new flight attendant service\"\n",
    "\n",
    "# Use the tokenizer to process the text\n",
    "encoding = tokenizer(sample_text, \n",
    "                     padding='max_length', \n",
    "                     truncation=True, \n",
    "                     max_length=64, \n",
    "                     return_tensors='pt')\n",
    "\n",
    "print(\"Original Sentence:\\n\", sample_text)\n",
    "print(\"\\nToken IDs (input_ids):\\n\", encoding['input_ids'])\n",
    "print(\"\\nAttention Mask:\\n\", encoding['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20182ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "sample_df = df.sample(n=16000, random_state=42)\n",
    "\n",
    "MAX_LEN = 64\n",
    "sentiment_dataset = SentimentDataset(\n",
    "    texts=sample_df.text.to_numpy(),\n",
    "    labels=sample_df.sentiment.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49946c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "\n",
    "train_size = int(0.8 * len(sentiment_dataset))\n",
    "test_size = len(sentiment_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(sentiment_dataset, [train_size, test_size])\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "  train_dataset,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=True\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "  test_dataset,\n",
    "  batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in the training loader: {len(train_data_loader)}\")\n",
    "print(f\"Number of batches in the testing loader: {len(test_data_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'accuracy': []\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "EPOCHS = 6\n",
    "\n",
    "with mlflow.start_run(run_name=\"DistilBERT\") as run:\n",
    "    mlflow.log_params({\"model_name\": model_name, \"epochs\": EPOCHS, \"learning_rate\": 2e-5, \"batch_size\": BATCH_SIZE, \"max_len\": MAX_LEN})\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'--- Epoch {epoch + 1} / {EPOCHS} ---')\n",
    "        \n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_data_loader, desc=\"Training\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            loss.backward()\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_data_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
    "\n",
    "        print(\"Evaluating...\")\n",
    "        model.eval()\n",
    "    \n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            batch_predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            predictions.extend(batch_predictions.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_accuracy = accuracy_score(true_labels, predictions)\n",
    "    history['accuracy'].append(val_accuracy)\n",
    "    \n",
    "    mlflow.log_metric(\"accuracy\", val_accuracy, step=epoch)\n",
    "\n",
    "    report = classification_report(true_labels, predictions, target_names=['Non-négatif (0)', 'Négatif (1)'], output_dict=True)\n",
    "    print(f\"\\nRésultats pour l'Epoch {epoch + 1}:\")\n",
    "    print(report)\n",
    "\n",
    "    mlflow.log_param(\"model_type\", \"DistilBERT\")\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    cleaned_report = {}\n",
    "    for key, value in report.items():\n",
    "        if isinstance(value, dict):\n",
    "            cleaned_key = key.replace(' (0)', '_0').replace(' (1)', '_1').replace('-', '_')\n",
    "            cleaned_report[cleaned_key] = value\n",
    "        else:\n",
    "            cleaned_report[key] = value\n",
    "\n",
    "    mlflow.log_metrics({f\"precision_{k}\": v['precision'] for k, v in cleaned_report.items() if isinstance(v, dict)})\n",
    "    mlflow.log_metrics({f\"recall_{k}\": v['recall'] for k, v in cleaned_report.items() if isinstance(v, dict)})\n",
    "    mlflow.log_metrics({f\"f1_score_{k}\": v['f1-score'] for k, v in cleaned_report.items() if isinstance(v, dict)})\n",
    "\n",
    "    mlflow.log_text(classification_report(true_labels, predictions, target_names=['Non-négatif (0)', 'Négatif (1)']), \"classification_report_distilbert.txt\")\n",
    "\n",
    "    print(f\"Logged DistilBERT metrics to MLflow (run_id={run.info.run_id}) - accuracy={accuracy:.4f}\")\n",
    "\n",
    "    if epoch == EPOCHS - 1:\n",
    "        model_artifact = {\"model\": model, \"tokenizer\": tokenizer}\n",
    "        mlflow.transformers.log_model(\n",
    "            transformers_model=model_artifact,\n",
    "            name=\"distilbert_sentiment_model\",\n",
    "            task=\"text-classification\"\n",
    "        )\n",
    "\n",
    "print(\"\\nEntraînement terminé et modèle sauvegardé avec MLflow !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb0029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.plot(history['train_loss'], label='Perte d entraînement')\n",
    "ax1.set_title('Évolution de la Perte')\n",
    "ax1.set_xlabel('Épochs')\n",
    "ax1.set_ylabel('Perte')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history['accuracy'], label='Précision de validation', color='orange')\n",
    "ax2.set_title('Évolution de la Précision')\n",
    "ax2.set_xlabel('Épochs')\n",
    "ax2.set_ylabel('Précision')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4297669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"./sentiment_distilbert_model/\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Sauvegarde du modèle dans le répertoire {output_dir}\")\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"\\nModèle et tokenizer sauvegardés avec succès !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ef5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "output_dir = \"./sentiment_distilbert_model/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "new_tweet = \"The flight was surprisingly on time and the crew was wonderful.\"\n",
    "\n",
    "inputs = tokenizer(new_tweet, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "prediction = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "sentiments = ['Négatif (0)', 'Positif (1)']\n",
    "print(f\"Le tweet : '{new_tweet}'\")\n",
    "print(f\"Result: {prediction}\")\n",
    "print(f\"Prédiction : {sentiments[prediction]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mlflow-serving-title",
   "metadata": {},
   "source": [
    "## 5. Chargement et Service du Modèle avec MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mlflow-serving-intro",
   "metadata": {},
   "source": [
    "Maintenant que nous avons entraîné et sauvegardé nos modèles avec MLflow, voyons comment les réutiliser. Nous allons charger le modèle DistilBERT (qui est le plus performant) depuis le tracking server de MLflow et l'utiliser pour faire une prédiction. Ensuite, nous verrons comment le déployer en tant que service local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlflow-load-predict",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "\n",
    "runs = mlflow.search_runs(experiment_names=[experiment_name], order_by=[\"start_time DESC\"], max_results=1)\n",
    "last_run_id = runs.iloc[0]['run_id']\n",
    "\n",
    "model_uri = f\"runs:/{{last_run_id}}/distilbert_sentiment_model\"\n",
    "\n",
    "loaded_model = mlflow.pyfunc.load_model(model_uri)\n",
    "\n",
    "new_tweet = \"This is the worst flight experience I have ever had.\"\n",
    "prediction_data = pd.DataFrame([new_tweet], columns=[\"text\"])\n",
    "\n",
    "prediction = loaded_model.predict(prediction_data)\n",
    "\n",
    "sentiments = ['Négatif (0)', 'Positif (1)']\n",
    "print(f\"Tweet: '{{new_tweet}}'\")\n",
    "print(f\"Prédiction : {sentiments[prediction[0]]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mlflow-serving-cli",
   "metadata": {},
   "source": [
    "### Démarrer un serveur de prédiction local\n",
    "\n",
    "MLflow facilite également le déploiement d'un modèle en tant que service REST API. Pour démarrer un serveur local qui sert notre modèle DistilBERT, vous pouvez exécuter la commande suivante dans votre terminal (en vous assurant que l'environnement où MLflow est installé est activé) :\n",
    "\n",
    "```bash\n",
    "# La variable MLFLOW_TRACKING_URI doit pointer vers votre serveur de suivi\n",
    "export MLFLOW_TRACKING_URI=file:/home/samuel/mlflow_data\n",
    "\n",
    "# Servez le modèle à partir de son URI\n",
    "mlflow models serve -m \"runs:/<RUN_ID>/distilbert_sentiment_model\" -p 1234\n",
    "```\n",
    "\n",
    "Remplacez `<RUN_ID>` par l'ID de l'exécution que vous souhaitez servir (par exemple, celui que nous avons récupéré dans la cellule précédente). Une fois le serveur démarré, vous pouvez lui envoyer des requêtes POST pour obtenir des prédictions, par exemple avec `curl` :\n",
    "\n",
    "```bash\n",
    "curl -X POST -H \"Content-Type:application/json\" --data '{\"dataframe_split\": {\"columns\":[\"text\"], \"data\":[[ \"I love MLflow!\"]]}}' http://127.0.0.1:1234/invocations\n",
    "```\n",
    "\n",
    "Cela conclut notre tour d'horizon de l'intégration de MLflow pour le suivi, la gestion et le déploiement de modèles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
