{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "652003cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de la bibliothèque essentielle pour la manipulation de données\n",
    "import pandas as pd\n",
    "\n",
    "# On définit des noms de colonnes clairs\n",
    "column_names = ['sentiment', 'id', 'date', 'query', 'user', 'text']\n",
    "\n",
    "# Chargement des données dans un \"DataFrame\"\n",
    "df = pd.read_csv(\"../data/training.1600000.processed.noemoticon.csv\", names=column_names, encoding=\"latin-1\")\n",
    "\n",
    "# On remplace la valeur 4 par 1 dans la colonne 'sentiment' pour la rendre binaire (0 ou 1)\n",
    "df['sentiment'] = df['sentiment'].replace(4, 1)\n",
    "\n",
    "# Affichage des 5 premières lignes pour un premier aperçu\n",
    "print(\"Aperçu des 5 premières lignes du tableau de données :\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mlflow-setup",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.transformers\n",
    "\n",
    "# Configuration de l'URI de suivi pour MLflow\n",
    "mlflow.set_tracking_uri(\"file:/mlflow\")\n",
    "\n",
    "# Définition du nom de l'expérience\n",
    "experiment_name = \"Analyse de Sentiments - Twitter\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"MLflow configuré pour l'expérience: '{experiment_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7568ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the DataFrame to 16000 elements, stratified by column 0\n",
    "n_samples = 16000\n",
    "\n",
    "# Sample 8000 rows where sentiment is 0 (negative sentiment)\n",
    "df_neg = df[df['sentiment'] == 0].sample(n=n_samples // 2, random_state=42)\n",
    "\n",
    "# Sample 8000 rows where sentiment is 1 (positive sentiment)\n",
    "df_pos = df[df['sentiment'] == 1].sample(n=n_samples // 2, random_state=42)\n",
    "\n",
    "# Concatenate the two dataframes\n",
    "df_truncated_stratified = pd.concat([df_neg, df_pos])\n",
    "\n",
    "# Shuffle the truncated DataFrame\n",
    "df_truncated_stratified = df_truncated_stratified.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Select only the columns we need (sentiment and text)\n",
    "df_prepared = df_truncated_stratified[['sentiment', 'text']]\n",
    "\n",
    "\n",
    "# Display the shape of the truncated and stratified DataFrame\n",
    "print(\"Shape of truncated and stratified DataFrame:\")\n",
    "print(df_prepared.shape)\n",
    "\n",
    "# Display the first few rows of the truncated and stratified DataFrame\n",
    "print(\"\\nAperçu du DataFrame tronqué et stratifié:\")\n",
    "display(df_prepared.head())\n",
    "\n",
    "# Check the value counts of the sentiment column to confirm stratification\n",
    "print(\"\\nValue counts of sentiment in truncated and stratified DataFrame:\")\n",
    "print(df_prepared['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3241b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_prepared\n",
    "# 1. Obtenir les dimensions du tableau (nombre de lignes, nombre de colonnes)\n",
    "print(f\"Dimensions du tableau : {df.shape}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 2. Obtenir un résumé des informations (types de données, valeurs non nulles)\n",
    "print(\"Informations sur le DataFrame :\")\n",
    "df.info()\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 3. Compter le nombre de tweets pour chaque sentiment\n",
    "print(\"Distribution des sentiments :\")\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217cef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the same powerful libraries as before\n",
    "import os\n",
    "\n",
    "# Disable the progress bar from huggingface_hub to avoid LookupError in some notebook environments\n",
    "os.environ[\"HF_HUB_DISABLE_PROGRESS_BARS\"] = \"1\"\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# The correct, English-based model name\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# 1. Load the Tokenizer for DistilBERT\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 2. Load the DistilBERT model for sequence classification (with 2 labels)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# --- BitFit implementation ---\n",
    "# Freeze all parameters by default\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze biases and the classification layer\n",
    "for name, param in model.named_parameters():\n",
    "    if 'bias' in name or 'classifier' in name or 'pre_classifier' in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Count and print the number of trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Number of trainable parameters (BitFit): {trainable_params}')\n",
    "\n",
    "print(\"DistilBERT model and its tokenizer are loaded and ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf7b732",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A sample tweet from our dataset\n",
    "sample_text = \"i love the new flight attendant service\"\n",
    "\n",
    "# Use the tokenizer to process the text\n",
    "encoding = tokenizer(sample_text, \n",
    "                     padding='max_length', \n",
    "                     truncation=True, \n",
    "                     max_length=64, \n",
    "                     return_tensors='pt')\n",
    "\n",
    "print(\"Original Sentence:\\n\", sample_text)\n",
    "print(\"\\nToken IDs (input_ids):\\n\", encoding['input_ids'])\n",
    "print(\"\\nAttention Mask:\\n\", encoding['attention_mask'])\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20182ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "sample_df = df.sample(n=16000, random_state=42)\n",
    "\n",
    "MAX_LEN = 64\n",
    "sentiment_dataset = SentimentDataset(\n",
    "    texts=sample_df.text.to_numpy(),\n",
    "    labels=sample_df.sentiment.to_numpy(),\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=MAX_LEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49946c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "\n",
    "train_size = int(0.8 * len(sentiment_dataset))\n",
    "test_size = len(sentiment_dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(sentiment_dataset, [train_size, test_size])\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_data_loader = DataLoader(\n",
    "  train_dataset,\n",
    "  batch_size=BATCH_SIZE,\n",
    "  shuffle=True\n",
    ")\n",
    "\n",
    "test_data_loader = DataLoader(\n",
    "  test_dataset,\n",
    "  batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "print(f\"Number of batches in the training loader: {len(train_data_loader)}\")\n",
    "print(f\"Number of batches in the testing loader: {len(test_data_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9497e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'accuracy': []\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "\n",
    "EPOCHS = 6\n",
    "\n",
    "with mlflow.start_run(run_name=\"DistilBERT\") as run:\n",
    "    mlflow.log_params({\"model_name\": model_name, \"epochs\": EPOCHS, \"learning_rate\": 2e-5, \"batch_size\": BATCH_SIZE, \"max_len\": MAX_LEN})\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f'--- Epoch {epoch + 1} / {EPOCHS} ---')\n",
    "        \n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch in tqdm(train_data_loader, desc=\"Training\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            loss.backward()\n",
    "            total_train_loss += loss.item()\n",
    "            \n",
    "            optimizer.step()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_data_loader)\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "\n",
    "        mlflow.log_metric(\"train_loss\", avg_train_loss, step=epoch)\n",
    "\n",
    "        print(\"Evaluating...\")\n",
    "        model.eval()\n",
    "    \n",
    "    predictions, true_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_data_loader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            \n",
    "            logits = outputs.logits\n",
    "            batch_predictions = torch.argmax(logits, dim=1)\n",
    "            \n",
    "            predictions.extend(batch_predictions.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    val_accuracy = accuracy_score(true_labels, predictions)\n",
    "    history['accuracy'].append(val_accuracy)\n",
    "    \n",
    "    mlflow.log_metric(\"accuracy\", val_accuracy, step=epoch)\n",
    "\n",
    "    report = classification_report(true_labels, predictions, target_names=['Non-négatif (0)', 'Négatif (1)'], output_dict=True)\n",
    "    print(f\"\\nRésultats pour l'Epoch {epoch + 1}:\")\n",
    "    print(report)\n",
    "\n",
    "    mlflow.log_param(\"model_type\", \"DistilBERT\")\n",
    "\n",
    "    cleaned_report = {}\n",
    "    for key, value in report.items():\n",
    "        if isinstance(value, dict):\n",
    "            cleaned_key = key.replace(' (0)', '_0').replace(' (1)', '_1').replace('-', '_')\n",
    "            cleaned_report[cleaned_key] = value\n",
    "        else:\n",
    "            cleaned_report[key] = value\n",
    "\n",
    "    mlflow.log_metrics({f\"precision_{k}\": v['precision'] for k, v in cleaned_report.items() if isinstance(v, dict)})\n",
    "    mlflow.log_metrics({f\"recall_{k}\": v['recall'] for k, v in cleaned_report.items() if isinstance(v, dict)})\n",
    "    mlflow.log_metrics({f\"f1_score_{k}\": v['f1-score'] for k, v in cleaned_report.items() if isinstance(v, dict)})\n",
    "\n",
    "    mlflow.log_text(classification_report(true_labels, predictions, target_names=['Non-négatif (0)', 'Négatif (1)']), \"classification_report_distilbert.txt\")\n",
    "\n",
    "    # Use the computed val_accuracy variable (defined above) instead of the undefined 'accuracy'\n",
    "    print(f\"Logged DistilBERT metrics to MLflow (run_id={run.info.run_id}) - accuracy={val_accuracy:.4f}\")\n",
    "\n",
    "    if epoch == EPOCHS - 1:\n",
    "        model_artifact = {\"model\": model, \"tokenizer\": tokenizer}\n",
    "        mlflow.transformers.log_model(\n",
    "            transformers_model=model_artifact,\n",
    "            name=\"distilbert_sentiment_model\",\n",
    "            task=\"text-classification\"\n",
    "        )\n",
    "\n",
    "print(\"\\nEntraînement terminé et modèle sauvegardé avec MLflow !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7afb0029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "ax1.plot(history['train_loss'], label='Perte d entraînement')\n",
    "ax1.set_title('Évolution de la Perte')\n",
    "ax1.set_xlabel('Épochs')\n",
    "ax1.set_ylabel('Perte')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history['accuracy'], label='Précision de validation', color='orange')\n",
    "ax2.set_title('Évolution de la Précision')\n",
    "ax2.set_xlabel('Épochs')\n",
    "ax2.set_ylabel('Précision')\n",
    "ax2.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4297669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "output_dir = \"../models/\"\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Sauvegarde du modèle dans le répertoire {output_dir}\")\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "print(\"\\nModèle et tokenizer sauvegardés avec succès !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83ef5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "output_dir = \"../models/\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(output_dir)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "new_tweet = \"The flight was surprisingly on time and the crew was wonderful.\"\n",
    "\n",
    "inputs = tokenizer(new_tweet, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = outputs.logits\n",
    "prediction = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "sentiments = ['Négatif (0)', 'Positif (1)']\n",
    "print(f\"Le tweet : '{new_tweet}'\")\n",
    "print(f\"Result: {prediction}\")\n",
    "print(f\"Prédiction : {sentiments[prediction]}\" )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
